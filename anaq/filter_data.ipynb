{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: Questão 2 - C infelizmente não tenho conhecimento suficiente para responser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyodbc\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import DataFrame\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "driver = os.getenv('DRIVER_NAME')\n",
    "server_name = os.getenv('SERVER_NAME')\n",
    "database_name = os.getenv('DATABASE_NAME')\n",
    "trusted_connection = os.getenv('TRUSTED_CONNECTION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ETL_Anuario').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de conexão com o SQL Server\n",
    "conn = pyodbc.connect(f'DRIVER={driver};SERVER={server_name};DATABASE={database_name};TRUSTED_CONNECTION={trusted_connection}')\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar o arquivo TXT e transformar os dados\n",
    "def carregar_arquivo_txt(arquivo: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega um arquivo TXT delimitado por ponto e vírgula (;) e transforma os dados em um DataFrame do Spark.\n",
    "    \n",
    "    :param arquivo (str): Caminho do arquivo TXT a ser carregado.\n",
    "    \n",
    "    :return DataFrame: DataFrame contendo os dados do arquivo, com as colunas inferidas.\n",
    "    \n",
    "    A função usa a biblioteca PySpark para ler o arquivo e transformar em um DataFrame, \n",
    "    inferindo automaticamente os tipos das colunas.\n",
    "    \"\"\"\n",
    "    # Carregar o arquivo TXT para o Spark\n",
    "    df = spark.read.option(\"delimiter\", \";\").csv(arquivo, header=True, inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para inserir dados na tabela 'atracacao_fato'\n",
    "def inserir_atracacao(df_atracao: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Insere os dados transformados de atracação na tabela 'atracacao_fato' no SQL Server.\n",
    "    \n",
    "    :param df_atracao (DataFrame): DataFrame contendo os dados de atracação transformados.\n",
    "    \n",
    "    :return None: Não retorna nenhum valor. Apenas insere os dados no banco.\n",
    "    \n",
    "    A função itera sobre cada linha do DataFrame e insere os dados na tabela 'atracacao_fato',\n",
    "    com base nas colunas específicas mencionadas no código.\n",
    "    \"\"\"\n",
    "    mes_map = {\n",
    "        'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6,\n",
    "        'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12\n",
    "    }\n",
    "    \n",
    "    for row in df_atracao.collect():\n",
    "        # Converte as colunas de data no formato dd/mm/yyyy hh:mm:ss para datetime\n",
    "        def str_to_datetime(date_str):\n",
    "            return datetime.strptime(date_str, '%d/%m/%Y %H:%M:%S') if date_str else None\n",
    "\n",
    "        data_atracacao = str_to_datetime(row['Data Atracação'])\n",
    "        data_chegada = str_to_datetime(row['Data Chegada'])\n",
    "        data_desatracacao = str_to_datetime(row['Data Desatracação'])\n",
    "        data_inicio_operacao = str_to_datetime(row['Data Início Operação'])\n",
    "        data_termino_operacao = str_to_datetime(row['Data Término Operação'])\n",
    "        \n",
    "        # Calculando as colunas de tempo (em dias)\n",
    "        t_espera_atracacao = (data_desatracacao - data_atracacao).days if data_atracacao and data_desatracacao else None\n",
    "        t_espera_inicio_op = (data_inicio_operacao - data_atracacao).days if data_atracacao and data_inicio_operacao else None\n",
    "        t_operacao = (data_termino_operacao - data_inicio_operacao).days if data_inicio_operacao and data_termino_operacao else None\n",
    "        t_atracado = (data_chegada - data_atracacao).days if data_chegada and data_atracacao else None\n",
    "        t_estadia = (data_termino_operacao - data_chegada).days if data_chegada and data_termino_operacao else None\n",
    "        \n",
    "        # Convertendo Ano e Mês para formatos adequados para inserção no banco\n",
    "        ano = row['Ano']  # Ano já está no formato yyyy\n",
    "        mes = mes_map.get(row['Mes'].lower(), None)  # Mapeando o nome do mês para número\n",
    "        \n",
    "        # Executando o comando de inserção com as colunas calculadas\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO atracacao_fato\n",
    "            (IDAtracacao, Tipo de Navegação da Atracação, CDTUP, Nacionalidade do Armador, IDBerco, FlagMCOperacaoAtracacao,\n",
    "            Berço, Terminal, Porto Atracação, Município, Apelido Instalação Portuária, UF, Complexo Portuário, SGUF,\n",
    "            Tipo da Autoridade Portuária, Região Geográfica, Data Atracação, No da Capitania, Data Chegada, No do IMO,\n",
    "            Data Desatracação, TEsperaAtracacao, Data Início Operação, TesperaInicioOp, Data Término Operação, TOperacao,\n",
    "            Ano da data de início da operação, TEsperaDesatracacao, Mês da data de início da operação, TAtracado,\n",
    "            Tipo de Operação, TEstadia)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", row['IDAtracacao'], row['Tipo de Navegação da Atracação'], row['CDTUP'], row['Nacionalidade do Armador'],\n",
    "           row['IDBerco'], row['FlagMCOperacaoAtracacao'], row['Berço'], row['Terminal'], row['Porto Atracação'],\n",
    "           row['Município'], row['Apelido Instalação Portuária'], row['UF'], row['Complexo Portuário'], row['SGUF'],\n",
    "           row['Tipo da Autoridade Portuária'], row['Região Geográfica'], row['Data Atracação'], row['Nº da Capitania'],\n",
    "           row['Data Chegada'], row['Nº do IMO'], row['Data Desatracação'], t_espera_atracacao,\n",
    "           row['Data Início Operação'], t_espera_inicio_op, row['Data Término Operação'], t_operacao,\n",
    "           ano, row['TEsperaDesatracacao'], mes, t_atracado, row['Tipo de Operação'], t_estadia)\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserir_carga(df_carga: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Insere os dados transformados de carga na tabela 'carga_fato' no SQL Server.\n",
    "    \n",
    "    :param df_carga (DataFrame): DataFrame contendo os dados de carga transformados.\n",
    "    \n",
    "    :return None: Não retorna nenhum valor. Apenas insere os dados no banco.\n",
    "    \n",
    "    A função itera sobre cada linha do DataFrame e insere os dados na tabela 'carga_fato',\n",
    "    com base nas colunas específicas mencionadas no código.\n",
    "    \"\"\"\n",
    "    for row in df_carga.collect():\n",
    "        # Observações sobre alguns campos:\n",
    "        # Para carga conteinerizada, o valor para 'CDMercadoria' será o código das mercadorias dentro do contêiner.\n",
    "        # Carga não conteinerizada = Peso bruto; Carga conteinerizada = Peso sem contêiner\n",
    "        peso_liquido = row['PesoLiquidoCarga'] if row['FlagConteinerTamanho'] == 'conteinerizada' else row['PesoLiquidoCarga']\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO carga_fato\n",
    "            (IDCarga, IDAtracacao, Origem, Destino, CDMercadoria, Tipo Operação da Carga, Carga Geral Acondicionamento,\n",
    "            ConteinerEstado, Tipo Navegação, FlagAutorizacao, FlagCabotagem, FlagCabotagemMovimentacao, FlagConteinerTamanho,\n",
    "            FlagLongoCurso, FlagMCOperacaoCarga, FlagOffshore, FlagTransporteViaInterioir, Percurso Transporte em vias Interiores,\n",
    "            Percurso Transporte Interiores, STNaturezaCarga, STSH2, STSH4, Natureza da Carga, Sentido, TEU, QTCarga, VLPesoCargaBruta,\n",
    "            Ano da data de início da operação da atracação, Mês da data de início da operação da atracação)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", row['IDCarga'], row['IDAtracacao'], row['Origem'], row['Destino'], row['CDMercadoria'],\n",
    "           row['Tipo Operação da Carga'], row['Carga Geral Acondicionamento'], row['ConteinerEstado'], row['Tipo Navegação'],\n",
    "           row['FlagAutorizacao'], row['FlagCabotagem'], row['FlagCabotagemMovimentacao'], row['FlagConteinerTamanho'],\n",
    "           row['FlagLongoCurso'], row['FlagMCOperacaoCarga'], row['FlagOffshore'], row['FlagTransporteViaInterioir'],\n",
    "           row['Percurso Transporte em vias Interiores'], row['Percurso Transporte Interiores'], row['STNaturezaCarga'],\n",
    "           row['STSH2'], row['STSH4'], row['Natureza da Carga'], row['Sentido'], row['TEU'], row['QTCarga'],\n",
    "           row['VLPesoCargaBruta'], row['Ano da data de início da operação da atracação'], row['Mês da data de início da operação da atracação'])\n",
    "        \n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os arquivos TXT de Carga e Atracação\n",
    "arquivos_carga = ['drive/MyDrive/Colab_Notebooks/antaq/2021Carga.txt', 'drive/MyDrive/Colab_Notebooks/antaq/2022Carga.txt', 'drive/MyDrive/Colab_Notebooks/antaq/2023Carga.txt']\n",
    "arquivos_atracao = ['drive/MyDrive/Colab_Notebooks/antaq/2021Atracacao.txt', 'drive/MyDrive/Colab_Notebooks/antaq/2022Atracacao.txt', 'drive/MyDrive/Colab_Notebooks/antaq/2023Atracacao.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados de Carga\n",
    "df_carga_2021 = carregar_arquivo_txt(arquivos_carga[0])\n",
    "df_carga_2022 = carregar_arquivo_txt(arquivos_carga[1])\n",
    "df_carga_2023 = carregar_arquivo_txt(arquivos_carga[2])\n",
    "\n",
    "# Carregar os dados de Atracação\n",
    "df_atracao_2021 = carregar_arquivo_txt(arquivos_atracao[0])\n",
    "df_atracao_2022 = carregar_arquivo_txt(arquivos_atracao[1])\n",
    "df_atracao_2023 = carregar_arquivo_txt(arquivos_atracao[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar os dados de todos os anos\n",
    "df_carga = df_carga_2021.union(df_carga_2022).union(df_carga_2023)\n",
    "df_atracao = df_atracao_2021.union(df_atracao_2022).union(df_atracao_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamar as funções para inserir os dados nas tabelas\n",
    "inserir_atracacao(df_atracao)\n",
    "inserir_carga(df_carga)\n",
    "\n",
    "# Fechar a conexão\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
